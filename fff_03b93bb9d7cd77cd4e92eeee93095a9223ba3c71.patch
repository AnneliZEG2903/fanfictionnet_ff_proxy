diff --git a/fanficfare/configurable.py b/fanficfare/configurable.py
index 358a0491..abd94e59 100644
--- a/fanficfare/configurable.py
+++ b/fanficfare/configurable.py
@@ -195,6 +195,7 @@ def get_valid_set_options():
                'use_ssl_unverified_context':(None,None,boollist),
                'use_cloudscraper':(None,None,boollist),
                'use_basic_cache':(None,None,boollist),
+               'use_fanfictionnet_ff_proxy':(None,None,boollist),
 
                ## currently, browser_cache_path is assumed to be
                ## shared and only ffnet uses it so far
@@ -480,6 +481,7 @@ def get_valid_keywords():
                  'use_basic_cache',
                  'use_browser_cache',
                  'use_browser_cache_only',
+                 'use_fanfictionnet_ff_proxy',
                  'browser_cache_path',
                  'browser_cache_age_limit',
                  'user_agent',
@@ -977,8 +979,11 @@ class Configuration(ConfigParser):
             # save and re-apply cookiejar when make_new.
         if not self.fetcher or make_new:
             logger.debug("use_cloudscraper:%s"%self.getConfig('use_cloudscraper'))
+            logger.debug("use_fanfictionnet_ff_proxy:%s"%self.getConfig('use_fanfictionnet_ff_proxy'))
             if self.getConfig('use_cloudscraper',False):
                 fetchcls = fetcher.CloudScraperFetcher
+            elif self.getConfig('use_fanfictionnet_ff_proxy',False):
+                fetchcls = fetcher.FanFiction_FF_ProxyFetcher
             else:
                 fetchcls = fetcher.RequestsFetcher
             self.fetcher = fetchcls(self.getConfig,
diff --git a/fanficfare/fetcher.py b/fanficfare/fetcher.py
index 2a934898..dc2d511b 100644
--- a/fanficfare/fetcher.py
+++ b/fanficfare/fetcher.py
@@ -54,6 +54,8 @@ from requests_file import FileAdapter
 import cloudscraper
 from cloudscraper.exceptions import CloudflareException
 
+import socket
+
 from . import exceptions
 
 ## makes requests/cloudscraper dump req/resp headers.
@@ -515,6 +517,89 @@ class CloudScraperFetcher(RequestsFetcher):
             msg = unicode(cfe).replace(' in the opensource (free) version','...')
             raise exceptions.FailedToDownload('cloudscraper reports: "%s"'%msg)
 
+class FanFiction_FF_ProxyFetcher(RequestsFetcher):
+    def __init__(self,getConfig_fn,getConfigList_fn):
+        super(FanFiction_FF_ProxyFetcher,self).__init__(getConfig_fn,getConfigList_fn)
+
+    def proxy_request(self,url):
+        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        s.setblocking(True)
+        s.connect(("127.0.0.1", 8888)) #FIXME: make the port configurable
+        #logger.debug('Sending URL to socket')
+        sent = s.sendall(url.encode('utf-8'))
+        if sent == 0:
+            logging.debug('Connection lost during sending')
+
+        header_raw = s.recv(4096)
+        header = header_raw.split(b'$END_OF_HEADER$')[0].decode('utf-8')
+        size_expected = int(header.split('||')[0])
+        type_expected = header.split('||')[1]
+        logger.debug('Expecting %i bytes of %s', size_expected, type_expected)
+
+        chunks = []
+        bytes_recd = 0
+
+        while bytes_recd <= size_expected:
+            chunk = s.recv(4096)
+            #logger.debug('Receiving %i bytes from socket', bytes_recd)
+            #if len(chunk.split(b'$END_OF_HEADER$')) > 1:
+                # We have part of the header in our chunk!
+                #chunk = chunk.split(b'$END_OF_HEADER$')[1]
+            if chunk == b'':
+                logging.debug('connection closed by remote host')
+                break
+            chunks.append(chunk)
+            bytes_recd = bytes_recd + len(chunk)
+        logger.debug('closing connection after %i bytes', bytes_recd)
+
+        s.close()
+
+        if type_expected == 'text':
+            content = b''.join(chunks).decode("utf-8")
+
+        if type_expected == 'image':
+            content = b''.join(chunks)
+            #logger.debug('Got %i bytes of image', len(content))
+
+        if type_expected == 'binary':
+            raise NotImplementedError()
+
+        # return (type,expected_size,received_size,content_as_bytes)
+        return (type_expected,size_expected,bytes_recd,content)
+
+    def request(self,method,url,headers=None,parameters=None):
+        if method != 'GET':
+            raise NotImplementedError()
+
+        content = b'initial_data'
+        retry_count = 0
+        while (retry_count < 5): #FIXME: make the retry counter configurable
+            (type_expected,size_expected,received_size,content) = self.proxy_request(url)
+
+            if received_size == size_expected:
+                # Everything normal
+                retry_count = 0
+                break
+
+            # Truncated reply, log the issue
+            logger.error('truncated reply from proxy! Expected %i bytes, received %i! ' % (size_expected, received_size))
+
+            logger.debug('resetting the browser state')
+            self.proxy_request('http://www.example.com') # Loading a very simple website seem to 'fix' this
+            logger.debug('waiting 5 seconds to let the browser settle')
+            time.sleep(5)
+
+            retry_count += 1
+
+        if retry_count == 5:
+            # We exited the retry loop without any valid content,
+            raise exceptions.FailedToDownload('fanfictionnet_ff_proxy: truncated reply from proxy')
+
+        return FetcherResponse(content,
+                                   url,
+                                   False)
+
+
 # .? for AO3's ']' in param names.
 safe_url_re = re.compile(r'(?P<attr>(pass(word)?|name|login).?=)[^&]*(?P<amp>&|$)',flags=re.MULTILINE)
 def safe_url(url):
